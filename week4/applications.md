<script type="text/javascript" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>

<span style="color:pink">*이 글은 Coursera에서 Andrew Ng 교수님의 머신러닝 강의를 듣고 읽기 자료를 읽으면서 복습차원에서 개인적으로 요약한 글입니다.*<span>

# 인공신경망 - Applications

## XNOR 문제의 해결
강의에서는 간단한 신경망의 예시로 비트(0, 1)의 NOT, AND, OR, XOR, XNOR을 소개하고 있다. 

먼저 NOT 문제는 입력 데이터 $x_1$에 대하여 NOT 연산의 결과값을 예측하는 모델을 인경신경망으로 만든다.

강의자료를 보면 NOT을 퍼셉트론 하나를 이용하여 입력 데이터 $x_1$를 받아 bias 항을 추가하여 포워딩을 한다.\
![NOT 문제](/week4/image/or.png)

포워딩 식은 다음과 같다.

$h_\theta(x)=g(10-20x_1)$

가능한 $x_1$은 0, 1이므로 이 두 입력 데이터로 포워딩을 하면 그림의 표처럼 원하는 대로 분류가 잘 됨을 알 수 있다. 

AND, OR, XOR, XNOR의 문제는 좀 다르다. 먼저 피연산자가 두 개이므로 입력 데이터도 $x_1$, $x_2$ 두 개가 된다. 먼저 AND, OR의 각 경우를 그래프로 그려보면 다음과 같다.

<img src="/week4/image/and_or.jpg" width="40%" height="30%" title="AND and OR"></img>

이미지에서 X 표시는 라벨이 1이고 O 표시는 라벨이 0인 데이터이다. 이미지에서 알 수 있듯이 AND와 OR은 결정 경계를 직선으로 그을 수 있다. 따라서 퍼셉트론 하나로 충분히 해결이 가능하다. 

![AND 문제](/week4/image/and.png)

위의 강의 그림에서는 AND 문제의 해결을 위해 퍼셉트론의 가설식을 $h_\theta(x)=g(-30+20x_1+20x_2)$으로 하여 모델을 구성하였다. 즉 $\theta_{10}^{(1)}=-30$, $\theta_{11}^{(1)}=20$, $\theta_{12}^{(1)}=20$이 된다. 이를 각 입력에 대해 포워딩하면 표처럼 각 경우에 대해서 잘 분류가 됨을 알 수 있다.

![OR 문제](/week4/image/or.png)

위의 강의 그림에서는 OR 문제의 해결을 위해 퍼셉트론의 가설식을 $h_\theta(x)=g(-10+20x_1+20x_2)$으로 하여 모델을 구성하였다. 즉 $\theta_{10}^{(1)}=-10$, $\theta_{11}^{(1)}=20$, $\theta_{12}^{(1)}=20$이 된다. 이를 각 입력에 대해 포워딩하면 표처럼 각 경우에 대해서 잘 분류가 됨을 알 수 있다.


XOR, XNOR 문제는 좀 다르다. 아래의 강의 슬라이드는 XNOR 문제를 보여 주고 있다.

![XNOR 문제](/week4/image/xnor.png)

그림에서 볼 수 있듯이 XNOR 문제는 직선 결정경계선으로는 해결할 수 없다. 즉 앞에서 소개했던 여러 층의 신경망을 사용해야 한다.

다음 그림은 여러 층의 퍼셉트론으로 가설식을 세워서 XNOR을 해결하는 모습이다. 

![XNOR 해결](/week4/image/solveXNOR.png)

층 1에서는 입력 $x_1$, $x_2$와 bias를 받고 층 2에서는 입력을 총 두 개의 유닛으로 포워딩한다. 

첫 번째 포워딩 유닛인 $a_1^{(2)}$ (빨간색)은 입력에 대한 AND 연산을 수행한다. 앞에서 사용했던 AND 가설식을 이용하면 다음과 같다.

 $a_1^{(2)}=g(-30+20x_1+20x_2)$

두 번째 포워딩 유닛인 $a_2^{(2)}$ (하늘색)는 입력에 대해서 각각 NOT을 적용한 후 두 값을 AND 연산한다. 앞에서 사용했던 NOT과 AND 가설식을 사용하면 다음과 같이 수행이 가능하다.

$\begin{matrix}
a_2^{(2)} &=& g(-10+20(0.5-x_1+20(0.5-x_2))) \\
&=& g(10-20x_1-20x_2)
\end{matrix}$

층 3에서는 포워딩 유닛인 $a_1^{(3)}$ (연두색)을 계산한다. 이 층에서는 포워딩 유닛 $a_1^{(2)}$, $a_2^{(2)}$를 OR 연산한다. 앞에서 사용했던 OR 가설식을 이용하면 다음과 같이 수행이 가능하다.

$a_1^{(3)}=g(-10+20x_1+20x_2)$

그리고 이 층의 포워딩 결과인 $a_1^{(3)}$ 바로 결과인 $h_\theta(x)$가 된다. 실제로 포워딩을 해보면 그림의 테이블처럼 XNOR연산과 똑같이 분류가 진행됨을 알 수 있다.

위에서는 간단한 비트연산에 대해서만 신경망을 구성해보았으나 더 많은 퍼셉트론으로 신경망을 구성하게 되면 더 복잡한 분류 문제 또한 해결할 수 있게 된다. 강의에서는 숫자 필체 분류의 예시를 들었다. 

## 다중 클래스 분류
이전에는 다중 클래스 분류 해결을 위해 각 클래스에 대하여 각각 이진 로지스틱 회귀를 진행하여 클래스 수만큼의 로지스틱 모델을 훈련하였다. 

신경망에서의 다중 클래스 분류도 유사하다. 다음 강의 그림에서 이를 살펴볼 수 있다. 

![다중클래스](/week4/image/multiclass.png)

그림을 보면 마지막에 출력 층에서 클래스 수만큼의 포워딩 유닛이 있는 것을 알 수 있다. 마치 클래스 수만큼 이진 로지스틱 회귀를 진행하는 것과 유사한 방식이다. 

예를 들어서 보행자, 차, 오토바이, 트럭이라는 분류 클래스가 존재하면 각 클래스에 대하여 라벨 데이터는 다음과 같이 벡터로 표현될 수 있다. 

보행자: $\begin{bmatrix}1\\0\\0\\0\end{bmatrix}$, 차: $\begin{bmatrix}0\\1\\0\\0\end{bmatrix}$, 오토바이: $\begin{bmatrix}0\\0\\1\\0\end{bmatrix}$, 트럭: $\begin{bmatrix}0\\0\\0\\1\end{bmatrix}$