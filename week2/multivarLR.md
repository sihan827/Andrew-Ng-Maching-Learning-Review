<script type="text/javascript" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>

<span style="color:pink">*이 글은 Coursera에서 Andrew Ng 교수님의 머신러닝 강의를 듣고 읽기 자료를 읽으면서 복습차원에서 개인적으로 요약한 글입니다.*<span>

# 다변수 선형 회귀 - Multivariate Linear Regression

## 여러 개의 특성
실제 문제에서는 지난 강의에서의 집 문제처럼 특성이 $x$ 하나인 경우는 드물고 $x_1^{(i)}$, $x_2^{(i)}$, $\cdots$ , $x_n^{(i)}$처럼 여러 개의 특성이 입력값으로 들어간다. 예를 들어 집 문제에서 특성이 집의 크기뿐만 아니라 방의 개수, 층의 개수, 건축년도 등 여러 개의 특성이 데이터로 주어질 수 있다. 즉 여기서 notation을 확대하여 다음과 같이 정의한다.
- $m$ : 훈련 데이터의 개수
- $n$ : 특성의 개수
- $x$ : 입력 데이터/특성 행렬
- $y$ : 출력 데이터/라벨
- $x^{(i)}$ : $i$번째 입력 특성 벡터
- $x_j^{(i)}$ : $i$번째 입력 특성 벡터의 $j$번째 특성

즉 $n$에 따라 $\theta_j$의 개수도 달라지므로 다변수 선형 회귀에 대하여 가설 $h$를 다음처럼 세울 수 있다.

$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n$

이를 벡터로 표현하여 더 간략화할 수 있다.

$i$번째 입력 특성 벡터 $x^{(i)}=\begin{bmatrix}x_0^{(i)}\\x_1^{(i)}\\x_2^{(i)}\\\vdots\\x_n^{(i)}\end{bmatrix}$ ($(n+1)$차원 벡터)

파라미터 벡터 $\theta=\begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\\vdots\\\theta_n\end{bmatrix}$ ($(n+1)$차원 벡터)

$\theta_0$은 그대로 더하므로 $x_0^{(i)}=1$이다. 즉 가설은 다음과 같다.

$h_\theta(x)=\theta^Tx$

## 다변수 선형 회귀에서의 경사하강법
비용함수는 지난 강의에서의 MSE를 이용하며 벡터화된 $\theta$를 이용하면 다음과 같이 표현된다.

$J(\theta)={1\over2m}\sum_{i=1}^m (h_{(\theta)}(x^{(i)})-y^{(i)})^2$

변수 1개의 선형 회귀에서의 경사하강법 알고리즘에서는 $\theta_0$, $\theta_1$만 있으므로 해당 계수에 대해서만 편미분을 진행하여 값을 업데이트하면 됐지만 다변수에서는 각 $\theta$에 대하여 모두 편미분을 해야 한다. 위 식을 $\theta_j$에 대하여 편미분하면 다음과 같다.

${\partial\over\partial\theta_j}J(\theta)={1\over m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$

이를 경사하강법 알고리즘에 적용하면 다음과 같다.

$J$가 수렴할 때까지 계속 반복 :

$\theta_j := \theta_j-\alpha{1\over m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$\
($j=0,1,2,\cdots , n$)

위 알고리즘을 통해 $J$가 최소가 되는 $\theta$ 벡터를 구할 수 있다.

## 특성 스케일링 (Feature Scaling)
특성 스케일링은 특성값들을 비슷한 범위로 스케일링한다. 이는 경사하강법 진행시 $\theta$의 이동 방향을 결정하므로 매우 중요하다.\
![특성 스케일링](/week2/image/featurescaling.png)\
강의에서는 집 가격 문제를 예시로 들었다. $x_1$은 집의 크기이고 $x_2$는 침실의 개수이다. 
- 먼저 스케일링을 하지 않으면 $x_1$은 0~2000의 범위값을 가지고 $x_2$는 1~5의 범위값을 가진다. 이를 통해 비용함수를 계산하여 등고선 그래프를 그리면 왼쪽처럼 타원형 그래프가 그려진다. 그럼 경사하강법으로 최솟값으로 접근하기 어려워진다.
- 다음으로 간단하게 해당 특성의 최댓값으로 특성값을 나누어 0 ~ 1 사이로 스케일링하면 두 특성의 범위가 같으므로 비용함수의 등고선 그래프가 오른쪽처럼 원형으로 그려진다. 그럼 경사하강법으로 최솟값으로 접근하기 수월해진다.

특성 스케일링 기법은 여러 가지가 있는데 강의에서는 Mean Normalization을 소개하고 있다. 간단하게 특성의 평균을 0으로 만드는 스케일링 방법이며 추가로 특성값의 범위로 나누어주어 특성값의 범위를 -1과 1 사이에 있도록 스케일링한다. j번째 특성인 $x_j$에 Mean Normalization을 적용하는 식은 다음과 같다.

$x_j:={{x_j-avg(x_j)}\over max(x_j)-min(x_j)}$

## 학습율 (Learning Rate)
경사하강법 알고리즘에서 $\alpha$가 학습율이라는 것을 지난 강의에서 배웠고 적절한 값으로 선택해야 한다는 것도 배웠다. 적당한 학습율을 선택하기 위해서는 경사하강법을 진행하면서 $J(\theta)$값이 어떤지 그래프를 그려가며 디버깅을 해야 한다. 보통 J(\theta)값이 0.001 이하로 감소하면 $J(\theta)$가 수렴한다고 간주한다. 

지난 강의에서 배웠듯이 $\alpha$가 너무 작으면 수렴이 느리고 너무 크면 경사하강법 진행 초기에 $J(\theta)$값이 들쑥날쑥할 수 있고 심지어는 발산할 수도 있다. 강의에서는 $\alpha$를 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 $\cdots$ 순으로 키워보면서 $J(\theta)$의 그래프를 체크해 보는 것을 권장한다고 설명하고 있다.\
<img src="/week2/image/correctlr.png" width="40%" height="30%" title="Correct LR"></img>\
이런 식의 $J(\theta)$ 그래프가 이상적이다.

## 다항식 회귀 (Polynomial Regression)
이제까지 다룬 회귀는 모두 선형 회귀로 특성들에 대한 1차식을 다뤘다. 하지만 데이터의 분포가 항상 1차 함수로만 표현될 수는 없을 것이다.

예를 들어 특성이 집 사이즈 한 개인 집 가격 문제를 다시 보면 밑의 강의자료에서처럼 데이터가 직선에 맞지 않는 경우가 있을 수 있다. \
![다항식 회귀](/week2/image/polynomialreg.png)\
이 경우 특성에 거듭제곱 등의 변화를 가해서 새로운 특성을 만들어내어 고차식으로 가설을 표현함으로써 데이터에 적합한 회귀 형태를 만들 수 있다. 위의 경우 특성에 제곱근을 씌우면 가장 적절한 형태의 회귀가 가능하다. 가설 $h$는 다음과 같이 세울 수 있다.

$h_\theta(x)=\theta_0+\theta_1x+\theta_2\sqrt x$
