<script type="text/javascript" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>

<span style="color:pink">*이 글은 Coursera에서 Andrew Ng 교수님의 머신러닝 강의를 듣고 읽기 자료를 읽으면서 복습차원에서 개인적으로 요약한 글입니다.*<span>

# 인공신경망 - Backpropagation in Practice
이번 강의에서는 강의에서 사용하는 프로그래밍 언어인 OCTAVE로 신경망을 구현하기 위한 몇 가지 팁을 소개하고 있다. 이는 비단 OCTAVE 뿐만 아니라 Python 등 다른 프로그래밍 언어로 신경망을 구현할 때에도 도움이 될 것이라고 생각된다.

## 파라미터 행렬 펼치기
OCTAVE로 비용 함수 최적화를 하려면 fminunc() 등의 내장 함수를 사용한다. 이 함수는 비용 함수 식, 파라미터 벡터, 옵션을 입력 인자로 받는다. 즉 최적화 함수 사용을 위해서 신경망의 파라미터들을 벡터로 길게 펼칠 필요가 있다. 

예를 들어서 레이어 4층짜리 신경망이 있으면 파라미터 행렬이 $\theta^{(1)}$,$\theta^{(2)}$, $\theta^{(3)}$ 세 개인데 이를 길게 늘여서 쓴다는 의미이다. $s_1=10$, $s_2=10$, $s_3=10$, $s_4=10$이면 $\theta^{(1)}$는 $(10\times11)$ 행렬, $\theta^{(2)}$는 $(10\times11)$ 행렬, $\theta^{(3)}$는 $(1\times11)$ 행렬이 된다. 각 파라미터 행렬을 전부 펼치므로 결과적으로 $\theta$의 벡터는 $(10\times11)+(10\times11)+(1\times11)=231$차원의 벡터가 된다. 각 파라미터에 대해 계산하는 편미분값에 대한 행렬 $D^{(1)}$, $D^{(2)}$, $D^{(3)}$ 또한 벡터로 펼쳐서 나타내어진다. 

이 펼쳐진 벡터를 이용해서 신경망 모델에 대한 비용함수 수행과정은 다음과 같다. 
- 비용함수는 파라미터 $\theta$ 벡터를 입력 인자로 받아 이를 각각 $\theta^{(l)}$ 행렬로 reshape한다. 
- 각 파라미터 행렬에 대해 순전파/역전파를 수행하여 편미분 행렬 $D^{(l)}$을 계산한다.
- 이를 $D$ 벡터로 펼쳐서 함수의 반환값으로 반환한다.

## 미분값 체크
강의에서 배운 역전파 알고리즘 식을 이용하면 역전파를 코드로 구현하는 것도 어렵지 않아 보이지만 구현이 올바르게 되었는지 확인할 필요가 있다. 구현했는데 비용함수의 값이 경사하강법을 진행하면서 감소한다면 역전파 알고리즘이 제대로 감소했다고 여길 수 있지만 사실 잘못 구현되어서 버그가 있을 수도 있다. 이를 더 정확하게 확인하려면 경사하강법 시작 전에 데이터에 대한 편미분값을 추정하여 역전파 알고리즘으로 구한 편미분값과 매우 비슷한지 체크해야 한다.

편미분값을 추정하는 것은 간단하다. 미분의 정의를 이용하여 매우 작은 구간에서의 기울기를 구하면 된다. 아래 슬라이드에서 이에 대한 설명이 있다.

![미분값 추정 공식](/week5/image/gradcheck.png)

그림에서 볼 수 있듯이 기존의 파라미터 값에 미세한 $\epsilon$ 값을 더하고 빼서 그 구간에 대한 기울기를 구하면 이는 원래 편미분값하고 유사할 것이다.

이를 모든 파라미터들에 대하여 수행한다. 파라미터 행렬들을 모두 펼친 파라미터 벡터 $\theta=[\theta_1, \theta_2, \cdots, \theta_n]$에 대하여 위의 공식으로 각각 편미분값을 추정한다. 이런 식으로 수행한다.

${\partial\over\partial\theta_1}\approx {J(\theta_1+\epsilon, \theta_2, \cdots, \theta_n)-J(\theta_1-\epsilon, \theta_2, \cdots, \theta_n)\over2\epsilon}$

${\partial\over\partial\theta_2}\approx {J(\theta_1, \theta_2+\epsilon, \cdots, \theta_n)-J(\theta_1, \theta_2-\epsilon, \cdots, \theta_n)\over2\epsilon}$

$\vdots$

${\partial\over\partial\theta_1}\approx {J(\theta_1, \theta_2, \cdots, \theta_n+\epsilon)-J(\theta_1, \theta_2, \cdots, \theta_n-\epsilon)\over2\epsilon}$

이후 이를 실제 역전파로 구한 편미분값들과 비교하여 값이 소숫점으로 얼마 차이 안 나면 역전파 알고리즘이 올바르게 구현되었다고 할 수 있다. 위의 편미분값 추정은 for 루프로 쉽게 구현이 가능하다.\
위의 미분값 체크는 처음 역전파 알고리즘을 수행할 때만 비교하고 실제 훈련시에는 이를 수행하지 않는다. 그 이유는 실제 훈련 시에도 최적화 알고리즘 반복 때마다 미분값을 체크하려면 그만큼 훈련 자체가 느려지기 때문이다. 

## 파라미터 랜덤 초기화
이전 선형회귀나 로지스틱 회귀 등에서는 모든 파라미터를 0으로 초기화하였다. 그러나 신경망은 모든 파라미터를 0으로 초기화하면 신경망의 장점을 잃어버린다. 아래 슬라이드에서 이를 잘 설명하고 있다. 

![0 초기화](/week5/image/zeroinit.png)

위 신경망에서 $\theta^{(1)}$을 초기화 할 때 모든 파라미터가 0으로 초기화되었다고 생각해 보면 $\theta_{01}^{(1)}=\theta_{02}^{(1)}$, $\theta_{11}^{(1)}=\theta_{12}^{(1)}$, $\theta_{21}^{(1)}=\theta_{22}^{(1)}$이므로 $a_1^{(2)}=a_2^{(2)}$가 된다. 이는 역전파 시 $\delta_1^{(1)}=\delta_2^{(1)}$가 되서 ${\partial\over\partial\theta_{01}^{(1)}}J(\theta)={\partial\over\partial\theta_{02}^{(1)}}J(\theta), {\partial\over\partial\theta_{11}^{(1)}}J(\theta)={\partial\over\partial\theta_{12}^{(1)}}J(\theta), {\partial\over\partial\theta_{21}^{(1)}}J(\theta)={\partial\over\partial\theta_{22}^{(1)}}J(\theta)$이 된다. 이는 훈련 시에 서로 같은 미분값을 빼게 되는 것이므로 훈련이 계속 진행되도 $\theta_{11}^{(1)}=\theta_{12}^{(1)}$, $\theta_{21}^{(1)}=\theta_{22}^{(1)}$는 계속 유지된다. 이러면 레이어 2의 유닛을 2개로 나눈 이유가 없어지는 셈이다.

따라서 파라미터들을 모두 0으로 초기화한다거나 같은 값으로 초기화하는 것은 의미가 없다. 따라서 모든 파라미터들을 랜덤으로 설정하는 것이 제일 바람직하다. 

일반적으로 임의의 값 $\epsilon$을 정하고 $-\epsilon\le\theta_{ij}^{(l)}\le\epsilon$을 만족하는 파라미터를 랜덤으로 뽑는 식으로 초기화시킨다.

## 신경망 학습 알고리즘 구현

이제 위의 팁들을 이용하여 신경망을 학습하는 알고리즘을 구현할 수 있다. 

먼저 신경망의 구조를 선택해야 한다. 
- 레이어 1과 마지막 레이어는 입력/출력 레이어이므로 훈련 데이터 x의 특성 개수 및 클래스 개수에 의해 정해진다.
- 은닉 레이어의 경우 보통 한 개를 사용하거나 하나 이상일 경우에는 모든 은닉 레이어가 동일한 개수의 퍼셉트론 유닛을 가지도록 설정하는 것을 기본으로 한다. 물론 은닉 레이어나 유닛 수가 많을수록 더 정확한 신경망 분류기가 될 것이다. (사실 이는 과적합 등의 문제를 야기한다. 이를 막기 위한 기법으로 Dropout 등이 존재한다.) 
  
이제 설정한 모델을 학습시켜야 한다.
1. 파라미터들을 랜덤으로 초기화시킨다.
2. 모든 훈련 데이터 $x^{(i)}$에 대한 $h_\theta(x^{(i)})$를 계산할 수 있도록 순전파 알고리즘을 구현한다.
3. 비용함수 $J(\theta)$를 계산할 수 있는 함수를 구현한다.
4. 비용함수를 모든 파라미터로 각각 편미분하여 ${\partial\over\partial\theta_{(jk)}^{(l)}}J(\theta)$를 계산할 수 있는 역전파 알고리즘을 구현한다. (이전 강의에서 배웠던 $\delta$를 계산하여 구현할 수 있음)
5. 구현한 순전파 알고리즘 및 역전파 알고리즘을 모든 훈련 데이터들에 대하여 시행하여 파라미터들의 편미분값을 계산한다.
6. 모든 훈련 데이터들에 대하여 편미분값 추정을 시행하여 5에서 계산한 편미분값과 유사한지 체크한다. (유사하지 않는다면 위의 단계에서 구현에 버그가 있는 것이다.)
7. 경사하강법이나 고급 최적화 기법을 이용하여 신경망을 여러 반복으로 학습시켜 비용함수 $J(\theta)$가 최소가 되도록 한다.

![최적화 그래프](/week5/image/optimization.png)

사실 파라미터가 매우 많기 때문에 3차원 그래프로 절대로 비용함수 값을 그릴 수 없다. 위의 그림은 파라미터 두 개에 대해서만 임의로 그린 것이다. 위 그래프처럼 $J(\theta)$는 여러 개의 일차식과 활성화함수의 합성식이므로 볼록 함수가 아니다. 따라서 곳곳에 local minimum이 산재해있다. 강의에서는 일단 local minimum에 빠지더라도 신경망 자체의 성능이 어느정도는 나온다고 설명한다. 물론 local minimum에 빠지는 문제 해결을 위해 다양한 고급 최적화 기법이 연구되고 있다.

## 인공신경망 연구의 예시

신경망을 실생활에 적용시키는 예시로써 자동차 자율주행이 있다. 강의에서는 간단한 자율주행 자동차 연구 예시를 보여준다. 먼저 카메라를 이용해 길을 촬영하면서 결과값으로 운전자의 핸들 회전 정도를 기록한다. 즉 입력은 도로의 사진이고 출력은 핸들 회전 정도가 된다. 이를 통해 학습시키면 해당 신경망은 길의 이미지를 정해진 프레임마다 입력으로 받아 핸들의 회전 정도를 결정하게 된다.